{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b679b64a-fde4-492c-a2be-78c1c68ba2fa",
   "metadata": {},
   "source": [
    "# TITLE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab38521-f505-40c5-b56d-e4d0b36f6f9d",
   "metadata": {},
   "source": [
    "This notebook was prepared by: \n",
    "\n",
    "Sahil Nisar (sn3028@nyu.edu)\n",
    "\n",
    "Suniya Raza (sr5748@nyu.edu)\n",
    "\n",
    "Vinicius Moreira (vgm236@nyu.edu)\n",
    "\n",
    "Graduate School of Arts and Science (GSAS) at New York University (NYU)\n",
    "\n",
    "2022"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2accd4f7-2b08-411a-9016-ff9b99614eca",
   "metadata": {},
   "source": [
    "PLACEHOLDER FOR SUMMARY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd6165c-13b8-4cb1-a886-b4c11cb4dfcf",
   "metadata": {},
   "source": [
    "## 1. Introduction "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f99f56b-01fe-46b2-a8ca-5b6bf81723a6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "562b62d8-6e65-4ee9-a9a1-a02055049d18",
   "metadata": {},
   "source": [
    "## 2. Literature Review"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927abdfb-b4a3-40f2-a3c1-84500e9e8c62",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1ebbdc01-daa2-41cc-b81a-537c83ca76bb",
   "metadata": {},
   "source": [
    "## 3. Forecasting methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0cdfd6-11d0-4eb9-93cc-0f157f3ac1f8",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 3.1 Univariate Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826bd822-88f5-4539-9a44-aa980299463b",
   "metadata": {},
   "source": [
    "**Trailing 3-period average**: \n",
    "\n",
    "This simple estimator plays the role of a na√Øve benchmark. It is an average of three periods, which can be 3-months or 3-quarters, depending on the variable used in our analysis.\n",
    "\n",
    "\\begin{align*}\n",
    "  \\hat{\\mu}_{t+1} &= \\frac{1}{T}\\sum^{t=3}_{t=1} {\\hat\\mu_t}\\\\\n",
    "  &\\text{where } \\hat{\\mu}_{t} \\text{is the independent variable}\\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7cba34-69fb-4ad0-a1a9-5fcf4225fff3",
   "metadata": {},
   "source": [
    "**Exponential Smoothing**: \n",
    "\n",
    "A weighted average of lagged values, with weights decaying exponentially the longer the lag. Exponential smoothing takes into account all past data, whereas moving average only takes into account $k$ past data points.\n",
    "\n",
    "\n",
    "$$ X_{t+1} = \\alpha X_{t} + \\alpha (1- \\alpha) X_{t-1} + \\alpha (1- \\alpha)^2 X_{t-2} + {...} $$\n",
    "\n",
    "\n",
    "where $0 \\le \\alpha \\le 1 $ is the smoothing parameter. You choose how many lags to use. We will use four lags here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055e2f8b-1942-4190-aff3-f229d77a876a",
   "metadata": {},
   "source": [
    "**ARIMA**: \n",
    "\n",
    "A stochastic process $ \\{X_t\\} $ is called an *autoregressive moving\n",
    "average process*, or ARMA($ p,q $), if it can be written as\n",
    "\n",
    "\n",
    "<a id='equation-arma'></a>\n",
    "$$\n",
    "X_t = \\phi_1 X_{t-1} + \\cdots + \\phi_p X_{t-p} +\n",
    "    \\epsilon_t + \\theta_1 \\epsilon_{t-1} + \\cdots + \\theta_q \\epsilon_{t-q} \\tag{28.5}\n",
    "$$\n",
    "\n",
    "where $ \\{ \\epsilon_t \\} $ is white noise.\n",
    "\n",
    "In what follows we **always assume** that the roots of the polynomial $ \\phi(z) $ lie outside the unit circle in the complex plane.\n",
    "\n",
    "This condition is sufficient to guarantee that the ARMA($ p,q $) process is covariance stationary.\n",
    "\n",
    "In fact, it implies that the process falls within the class of general linear processes.\n",
    "\n",
    "We define an ARIMA(p, d, q) model as the mixture of an AR(p) and MA(q) model with differencing (to help make the process stationary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dad0ccd-512e-4b79-b47e-1db356dfda13",
   "metadata": {},
   "source": [
    "#### 3.2 Linear Regression and Machine Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de3c35f-7578-4bad-bc9c-18f2f8c21892",
   "metadata": {},
   "source": [
    "**Simple Linear Regression (OLS):**\n",
    "\n",
    "he most common technique to estimate a linear relationship between variables is Ordinary Least Squares (OLS). OLS model is solved by finding the parameters that minimize the sum of squared residuals.\n",
    "\n",
    "The model can be defined, in the matrix form, as:\n",
    "\n",
    "$$\n",
    "y = X\\beta + u\n",
    "$$\n",
    "\n",
    "To solve for the unknown parameter $ \\beta $, we want to minimize\n",
    "the sum of squared residuals\n",
    "\n",
    "$$\n",
    "\\underset{\\hat{\\beta}}{\\min} \\hat{u}'\\hat{u}\n",
    "$$\n",
    "\n",
    "Rearranging the first equation and substituting into the second\n",
    "equation, we can write\n",
    "\n",
    "$$\n",
    "\\underset{\\hat{\\beta}}{\\min} \\ (Y - X\\hat{\\beta})' (Y - X\\hat{\\beta})\n",
    "$$\n",
    "\n",
    "Solving this optimization problem gives the solution for the\n",
    "$ \\hat{\\beta} $ coefficients\n",
    "\n",
    "$$\n",
    "\\hat{\\beta} = (X'X)^{-1}X'y\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99dd5b16-78f6-4f58-8d8a-d489e18876c0",
   "metadata": {},
   "source": [
    "**Ridge / Lasso / Elastic Net:**\n",
    "\n",
    "These models are very closely related to traditional OLS, but they focus on regularization of parameters to avoid overfitting.\n",
    "\n",
    "The Lasso model generates predictions using but optimizes over a slightly different loss function:\n",
    "\n",
    "$$\n",
    "\\underset{\\hat{\\beta}}{\\min} \\ (Y - X\\hat{\\beta})' (Y - X\\hat{\\beta}) + \\alpha\\hat{\\beta}\n",
    "$$\n",
    "\n",
    "where $ \\alpha $ is the regularization parameter. The additional term penalizes large coefficients and in practice, effectively sets coefficients to zero for features that are not informative about the target.\n",
    "\n",
    "\n",
    "Ridge regressions places a particular form of constraint on the parameters $\\beta$, which is chosen to minimize the penalized sum of squares:\n",
    "\n",
    "$$\n",
    "\\underset{\\hat{\\beta}}{\\min} \\ (Y - X\\hat{\\beta})' (Y - X\\hat{\\beta}) + \\lambda\\hat{\\beta}'\\hat{\\beta}\n",
    "$$\n",
    "\n",
    "This means that if the $\\beta$ take on large values, the optimization function is penalized, but not zero (only reducing the impact of \"irrelevant\" features of the model).\n",
    "\n",
    "The elastic net algorithm uses a weighted combination of Ridge and Lasso forms of regularization. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea7d516-57b4-476d-917c-b936a10d8928",
   "metadata": {},
   "source": [
    "#### 3.3 More Complex Econometric Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996dff66-f05c-4c64-b6fc-c3f23e4bf155",
   "metadata": {},
   "source": [
    "**Dynamic Factor Model:**\n",
    "\n",
    "In a dynamic factor model, we model a potentially large number of macroeconomic series as being driven by a much smaller number of latent factors, which are estimated through a principal component analysis. \n",
    "\n",
    "Principal component analysis is an unsupervised algorithm, based on feature correlation, used for dimensionality reduction. The premise is simply to take data of higher dimensions, and reduce to a lower dimension.\n",
    "\n",
    "Often times, in higher dimensional data, it isn't possible to create visual representations of relationships between variables. Through applying PCA, it then becomes possible to reduce the dimensions of the data and display variable relationships. This tool also allows easier visualization and noise filtering, among other applications.\n",
    "\n",
    "The PCA must be used when three conditions apply:\n",
    "\n",
    "1. Reduce the number of variables\n",
    "2. Ensure that each variable is independent of one another\n",
    "3. Assume that the interpretation of the independent variables is less important\n",
    "\n",
    "How does a PCA work?\n",
    "\n",
    "a. Calculate a matrix that summarizes how the variables are related one another (the covariance matrix).\n",
    "\n",
    "b. Then separate it between direction (eigenvectors) and magnitude (eigenvalues)\n",
    "\n",
    "c. By projecting the data into a smaller space, we reduce dimension, but keep the original variables in our model\n",
    "\n",
    "Mathematically, the first principal component is the direction in space along which projections have the largest variance. The second principal component is the direction which maximizes variance among all directions orthogonal to the first. The k-th component is the variance-maximizing direction orthogonal to the previous k-1 components.\n",
    "\n",
    "With those principal components, we use them as explanatory variables in an OLS."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d887f6e4-1930-44ca-b3b5-61a61ae4fa04",
   "metadata": {},
   "source": [
    "**Vector autoregressions (VARs):**\n",
    "\n",
    "Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1c2ddd-f719-4789-a983-c285867f4519",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 3.4 Nonlinear Algorithms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8599892c-c67f-4613-b4b0-7bdafd87ac39",
   "metadata": {},
   "source": [
    "**Random Forest:**\n",
    "\n",
    "Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6c59f1-30cb-4e41-a68d-e3216147cd76",
   "metadata": {},
   "source": [
    "**Gradient Boosted Decision Trees:**\n",
    "\n",
    "Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a514e687-2a6e-4f3a-839d-2217a61593f8",
   "metadata": {},
   "source": [
    "**K-Nearest Neighbors:**\n",
    "\n",
    "Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b118452-6857-4898-8091-6d4b73d27d13",
   "metadata": {},
   "source": [
    "**Support Vector Regression:**\n",
    "\n",
    "Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46ea259-328a-40fb-becf-6b35822f0a29",
   "metadata": {},
   "source": [
    "#### 3.5 Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9e8f88-db50-4ef8-be8f-dcea92939a11",
   "metadata": {},
   "source": [
    "**Dense:**\n",
    "\n",
    "Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31bb8bf5-1775-4f07-9317-d5e6ab3dc56b",
   "metadata": {},
   "source": [
    "**LSTM:**\n",
    "\n",
    "Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf23602f-bd86-42da-9aa6-b7044d29ae34",
   "metadata": {},
   "source": [
    "## 4. Applications to US GDP growth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41fc0e6-74da-4443-a0c6-4154a450ddbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20c16f3-ac37-4de1-8636-0267be0b9deb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "daec3f2f-c9d3-47df-ada0-ca87d0f76fee",
   "metadata": {},
   "source": [
    "## 5. Applications to Brazil industrial production growth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c070e778-075f-4e01-8408-d3923f044c41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c3f1cc-0a4a-4c00-a44d-2727f4f2f063",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a6514574-2d55-4fd5-bb4d-ae9fc9f17332",
   "metadata": {},
   "source": [
    "## 6. Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4613effb-ddbe-4647-b24c-ec06cc4bbcee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d16ed3-1b21-48dd-b671-a0e4f3b65172",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f5956d-34d8-4535-b23d-93fd7a13131c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1a41b0c9-3fea-48c2-8de3-1e3ab3bc1fe7",
   "metadata": {},
   "source": [
    "## 7. References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc21434a-36bf-4cdb-b76e-a3229dea6d3b",
   "metadata": {},
   "source": [
    "References (make this more professional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f2e31a-c92f-428e-a187-4780b10ad839",
   "metadata": {},
   "source": [
    "https://machinelearningmastery.com/exponential-smoothing-for-time-series-forecasting-in-python/\n",
    "\n",
    "https://python-advanced.quantecon.org/arma.html\n",
    "\n",
    "https://www.bounteous.com/insights/2020/09/15/forecasting-time-series-model-using-python-part-two/\n",
    "\n",
    "Towards Science: A One-Stop Shop for Principal Component Analysis (https://towardsdatascience.com/a-one-stop-shop-for-principal-component-analysis-5582fb7e0a9c).\n",
    "\n",
    "In Depth: Principal Component Analysis (https://jakevdp.github.io/PythonDataScienceHandbook/05.09-principal-component-analysis.html).\n",
    "\n",
    "Advanced Data Analysis from an Elementary Point of View (https://www.stat.cmu.edu/~cshalizi/uADA/15/lectures/17.pdf).\n",
    "\n",
    "Applications of Principal Component Analysis (PCA) (https://iq.opengenus.org/applications-of-pca/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
